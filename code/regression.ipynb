{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3047, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3102, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3306, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3489, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3549, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/64/r59563zn47nfrpq05dsj39vc0000gn/T/ipykernel_71476/2170915313.py\", line 8, in <module>\n",
      "    import torch  # Main framework for defining and training the transformer\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/paolopichini/Desktop/Desktop - MacBook Pro di Paolo/Coding/ML/Solo/MLpractice/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# This is my first transformer project\n",
    "# The idea is to create a transformer that can take as imput a list (N mod p1, N mod p2, ..., N mod pn)\n",
    "# where N is a number (integer, rational, etc) and p1,p2,...,pn are prime numbers\n",
    "# and returns the number N\n",
    "\n",
    "# %% \n",
    "# Let us start by importing the necessary libraries\n",
    "import torch  # Main framework for defining and training the transformer\n",
    "import torch.nn as nn  # Neural network module\n",
    "import torch.optim as optim  # Optimization functions\n",
    "import numpy as np  # For numerical operations\n",
    "import random  # For generating random numbers\n",
    "import itertools  # (Optional) For generating structured datasets\n",
    "\n",
    "import matplotlib.pyplot as plt  # (Optional) For visualization\n",
    "from torch.utils.data import Dataset, DataLoader  # To handle training data efficiently\n",
    "\n",
    "import time # For timing the training process\n",
    "\n",
    "import json # For saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration:\n",
      "{'model_params': {'model': 'MLP', 'input_dim': 5, 'hidden_dim': 512, 'output_dim': 1}, 'training_params': {'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 100, 'optimizer': 'Adam'}, 'log_params': {'experiment_name': 'experiment_001', 'notes': 'Baseline experiment with MLP'}}\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from a JSON file\n",
    "with open(\"config1.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Access parameters like:\n",
    "input_dim = config[\"model_params\"][\"input_dim\"]\n",
    "hidden_dim = config[\"model_params\"][\"hidden_dim\"]\n",
    "output_dim = config[\"model_params\"][\"output_dim\"]\n",
    "\n",
    "learning_rate = config[\"training_params\"][\"learning_rate\"]\n",
    "batch_size = config[\"training_params\"][\"batch_size\"]\n",
    "num_epochs = config[\"training_params\"][\"num_epochs\"]\n",
    "\n",
    "print(\"Loaded configuration:\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen primes: [3, 7, 13, 19, 31]\n",
      "Product of primes (P): 160797\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now we move onto data generation\n",
    "# the idea is to first define a list of primes (p1,p2,...,pn)\n",
    "# then we pick integers N and compute the remainders of N when divided by each prime\n",
    "# then the pairs (N mod p1, N mod p2, ..., N mod pn) are stored as input and N is stored as output\n",
    "# this will form our training dataset\n",
    "\n",
    "# Define a list of small primes\n",
    "#primes = [2, 3, 5, 7, 11]\n",
    "primes = [3, 7, 13, 19, 31]\n",
    "\n",
    "# Compute the product P = p1 * p2 * ... * pn\n",
    "P = np.prod(primes)\n",
    "\n",
    "# Check what we have done so far\n",
    "print(f\"Chosen primes: {primes}\")\n",
    "print(f\"Product of primes (P): {P}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Input (moduli) tensor([-0.3333, -0.4286, -0.3846, -0.4737,  0.8710]), Output (N) 0.28164082765579224\n",
      "Sample 1: Input (moduli) tensor([-1.0000, -0.7143,  0.2308,  0.7895, -0.6774]), Output (N) -0.3841676115989685\n",
      "Sample 2: Input (moduli) tensor([ 0.3333, -0.7143, -0.8462, -0.6842,  0.2258]), Output (N) 0.7091239094734192\n",
      "Sample 0: Input (moduli) tensor([ 1.,  2.,  4.,  5., 29.]), Output (N) 103042.0\n",
      "Sample 1: Input (moduli) tensor([ 0.,  1.,  8., 17.,  5.]), Output (N) 49512.0\n",
      "Sample 2: Input (moduli) tensor([ 2.,  1.,  1.,  3., 19.]), Output (N) 137411.0\n",
      "Check first sample: [1.0, 2.0, 4.0, 5.0, 29.0] from 103042.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now we define a PyTorch Dataset to handle our data\n",
    "# This creates a class that inherits from the `Dataset` class in PyTorch\n",
    "# so that we can use PyTorch's `DataLoader` to load the data efficiently\n",
    "\n",
    "# Define a PyTorch Dataset for our data\n",
    "class ModuloDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, myprimes=None):\n",
    "        super(ModuloDataset, self).__init__() # Initialize the base class 'Dataset', not strictly required here but a good practice\n",
    "        self.myprimes = myprimes if myprimes else primes\n",
    "        self.P = np.prod(self.myprimes)\n",
    "        self.samples = []\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            N = random.randint(0, self.P - 1)  # Pick a random integer N\n",
    "            normalized_N = 2 * (N / self.P) - 1  # Normalize & center N\n",
    "            normalized_remainders = [2*(N % p)/p - 1 for p in self.myprimes]  # Compute remainders\n",
    "            self.samples.append((torch.tensor(normalized_remainders, dtype=torch.float32), \n",
    "                                 torch.tensor(normalized_N, dtype=torch.float32)))  # Convert to tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)  # Return the total number of samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]  # Return the (input, output) pair at index `idx`\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ModuloDataset(num_samples=10,myprimes=primes)\n",
    "\n",
    "# Check some rescaled samples\n",
    "for i in range(min(3,len(dataset))):\n",
    "    print(f\"Sample {i}: Input (moduli) {dataset[i][0]}, Output (N) {dataset[i][1]}\")\n",
    "\n",
    "# Check some samples without rescaling\n",
    "for i in range(min(3,len(dataset))):\n",
    "    print(f\"Sample {i}: Input (moduli) {(torch.tensor(primes)*dataset[i][0]+torch.tensor(primes))/2}, Output (N) {(torch.tensor(int(P))*dataset[i][1]+torch.tensor(int(P)))/2}\")\n",
    "\n",
    "# Check the samples above are consistent\n",
    "checksample0 = (torch.tensor(int(P))*dataset[0][1]+torch.tensor(int(P))).item()/2\n",
    "print(f\"Check first sample: {[checksample0 % p for p in primes]} from {checksample0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 10\n",
      "Batch Inputs (moduli): torch.Size([10, 5])\n",
      "Batch Inputs (moduli): tensor([[ 0.3333, -0.7143, -0.8462, -0.6842,  0.2258],\n",
      "        [-0.3333, -0.7143,  0.5385,  0.3684, -0.6129],\n",
      "        [-0.3333, -0.4286, -0.3846, -0.4737,  0.8710],\n",
      "        [-0.3333, -1.0000,  0.3846, -0.4737, -0.0968],\n",
      "        [-1.0000,  0.7143, -0.0769,  0.3684, -0.8065],\n",
      "        [-1.0000, -0.7143, -0.2308,  0.2632,  0.6129],\n",
      "        [-0.3333, -0.4286,  0.8462,  0.0526,  0.4839],\n",
      "        [-1.0000, -0.7143,  0.2308,  0.7895, -0.6774],\n",
      "        [ 0.3333, -0.1429, -0.0769, -0.3684,  0.5484],\n",
      "        [-1.0000,  0.1429,  0.6923,  0.3684, -0.7419]])\n",
      "Batch Targets (N values): torch.Size([10])\n",
      "Batch Targets (N values): tensor([ 0.7091,  0.4209,  0.2816,  0.5035,  0.4228,  0.3522, -0.7028, -0.3842,\n",
      "        -0.8647, -0.4520])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now we define the DataLoaders to load the data efficiently during training\n",
    "# This will allow us to load the data in batches, shuffle it, etc.\n",
    "\n",
    "# Create a DataLoader to load the dataset in batches\n",
    "batch_size = min(32, len(dataset))  # Number of samples per batch\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Check one batch of data\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch  # Unpack batch\n",
    "    print(f\"Batch Inputs (moduli): {inputs.shape}\")  # Should be (batch_size, num_primes)\n",
    "    print(f\"Batch Inputs (moduli): {inputs}\")  # Should be (batch_size, num_primes)\n",
    "    print(f\"Batch Targets (N values): {targets.shape}\")  # Should be (batch_size,)\n",
    "    print(f\"Batch Targets (N values): {targets}\")  # Should be (batch_size,)\n",
    "    break  # Only print the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now we define the transformer model\n",
    "# We use the nn.Module class in PyTorch to define our model\n",
    "# The transformer model consists of an input embedding layer, followed by a transformer encoder\n",
    "# and finally a linear layer to output the predicted value\n",
    "# Note: here we need to use super() or else some necessary initializations from nn.Module will be missed\n",
    "class ModuloTransformer(nn.Module):\n",
    "    def __init__(self, num_primes, d_model=128, num_heads=4, num_layers=2, hidden_dim=256):\n",
    "        super(ModuloTransformer, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(num_primes, d_model)  # Input embedding\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=hidden_dim),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, 1)  # Final output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Project input to d_model dimension\n",
    "        x = self.transformer_encoder(x)  # Pass through transformer layers\n",
    "        x = self.fc_out(x).squeeze(-1)  # Final output (scalar prediction)\n",
    "        return x\n",
    "    \n",
    "# Also define a simple MLP model for comparison\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden_dim=128, output_dim=1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x).squeeze(-1)  # No activation on output (for regression)\n",
    "        return x\n",
    "\n",
    "# define model\n",
    "#model = ModuloTransformer(num_primes=len(primes), d_model=256, num_heads=4, num_layers=2, hidden_dim=256)\n",
    "model = MLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Inputs (moduli):\n",
      "tensor([[-0.3333, -0.4286, -0.3846, -0.4737,  0.8710],\n",
      "        [ 0.3333, -0.1429, -0.0769, -0.3684,  0.5484],\n",
      "        [-1.0000, -0.7143,  0.2308,  0.7895, -0.6774],\n",
      "        [-1.0000,  0.1429,  0.6923,  0.3684, -0.7419],\n",
      "        [-1.0000,  0.7143, -0.0769,  0.3684, -0.8065],\n",
      "        [-0.3333, -0.7143,  0.5385,  0.3684, -0.6129],\n",
      "        [ 0.3333, -0.7143, -0.8462, -0.6842,  0.2258],\n",
      "        [-0.3333, -1.0000,  0.3846, -0.4737, -0.0968],\n",
      "        [-1.0000, -0.7143, -0.2308,  0.2632,  0.6129],\n",
      "        [-0.3333, -0.4286,  0.8462,  0.0526,  0.4839]])\n",
      "Model Output Before Training:\n",
      "tensor([ 0.0693,  0.0373,  0.0098, -0.0219, -0.0298, -0.0051,  0.0520,  0.0057,\n",
      "         0.0658,  0.0654], grad_fn=<SqueezeBackward1>)\n",
      "Expected Targets (N values):\n",
      "tensor([ 0.2816, -0.8647, -0.3842, -0.4520,  0.4228,  0.4209,  0.7091,  0.5035,\n",
      "         0.3522, -0.7028])\n",
      "Inputs shape: torch.Size([10, 5])\n",
      "Outputs shape: torch.Size([10])\n",
      "Targets shape: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Now we check the model before training\n",
    "\n",
    "# We begin by checking the output of the untrained model\n",
    "# Get a batch from the DataLoader\n",
    "test_inputs, test_targets = next(iter(dataloader))  # Get first batch\n",
    "# Run a forward pass through the model\n",
    "test_outputs = model(test_inputs)\n",
    "# Print input values\n",
    "print(\"Sample Inputs (moduli):\")\n",
    "print(test_inputs)\n",
    "# Print raw model predictions\n",
    "print(\"Model Output Before Training:\")\n",
    "print(test_outputs)\n",
    "# Print expected target values\n",
    "print(\"Expected Targets (N values):\")\n",
    "print(test_targets)\n",
    "# Print shape information\n",
    "print(f\"Inputs shape: {test_inputs.shape}\")  # Should be (batch_size, num_primes)\n",
    "print(f\"Outputs shape: {test_outputs.shape}\")  # Should be (batch_size,)\n",
    "print(f\"Targets shape: {test_targets.shape}\")  # Should match outputs (batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output After One Training Step:\n",
      "tensor([ 0.1953,  0.0301, -0.0878, -0.1462,  0.0515, -0.0039,  0.3470,  0.1609,\n",
      "         0.1694, -0.1073], grad_fn=<SqueezeBackward1>)\n",
      "Epoch 1/100, Loss: 0.1945, LR: 0.001\n",
      "Epoch 2/100, Loss: 0.1336, LR: 0.001\n",
      "Epoch 3/100, Loss: 0.0891, LR: 0.001\n",
      "Epoch 4/100, Loss: 0.0578, LR: 0.001\n",
      "Epoch 5/100, Loss: 0.0380, LR: 0.001\n",
      "Epoch 6/100, Loss: 0.0277, LR: 0.001\n",
      "Epoch 7/100, Loss: 0.0235, LR: 0.001\n",
      "Epoch 8/100, Loss: 0.0213, LR: 0.001\n",
      "Epoch 9/100, Loss: 0.0181, LR: 0.001\n",
      "Epoch 10/100, Loss: 0.0137, LR: 0.001\n",
      "Epoch 11/100, Loss: 0.0090, LR: 0.001\n",
      "Epoch 12/100, Loss: 0.0052, LR: 0.001\n",
      "Epoch 13/100, Loss: 0.0027, LR: 0.001\n",
      "Epoch 14/100, Loss: 0.0016, LR: 0.001\n",
      "Epoch 15/100, Loss: 0.0015, LR: 0.001\n",
      "Epoch 16/100, Loss: 0.0021, LR: 0.001\n",
      "Epoch 17/100, Loss: 0.0027, LR: 0.001\n",
      "Epoch 18/100, Loss: 0.0032, LR: 0.001\n",
      "Epoch 19/100, Loss: 0.0031, LR: 0.001\n",
      "Epoch 20/100, Loss: 0.0028, LR: 0.001\n",
      "Epoch 21/100, Loss: 0.0023, LR: 0.001\n",
      "Epoch 22/100, Loss: 0.0017, LR: 0.001\n",
      "Epoch 23/100, Loss: 0.0012, LR: 0.001\n",
      "Epoch 24/100, Loss: 0.0009, LR: 0.001\n",
      "Epoch 25/100, Loss: 0.0008, LR: 0.001\n",
      "Epoch 26/100, Loss: 0.0007, LR: 0.001\n",
      "Epoch 27/100, Loss: 0.0007, LR: 0.001\n",
      "Epoch 28/100, Loss: 0.0005, LR: 0.001\n",
      "Epoch 29/100, Loss: 0.0004, LR: 0.001\n",
      "Epoch 30/100, Loss: 0.0004, LR: 0.001\n",
      "Epoch 31/100, Loss: 0.0004, LR: 0.001\n",
      "Epoch 32/100, Loss: 0.0004, LR: 0.001\n",
      "Epoch 33/100, Loss: 0.0004, LR: 0.001\n",
      "Epoch 34/100, Loss: 0.0004, LR: 0.001\n",
      "Epoch 35/100, Loss: 0.0003, LR: 0.001\n",
      "Epoch 36/100, Loss: 0.0002, LR: 0.001\n",
      "Epoch 37/100, Loss: 0.0001, LR: 0.001\n",
      "Epoch 38/100, Loss: 0.0001, LR: 0.001\n",
      "Epoch 39/100, Loss: 0.0001, LR: 0.001\n",
      "Epoch 40/100, Loss: 0.0002, LR: 0.001\n",
      "Epoch 41/100, Loss: 0.0002, LR: 0.001\n",
      "Epoch 42/100, Loss: 0.0002, LR: 0.001\n",
      "Epoch 43/100, Loss: 0.0001, LR: 0.001\n",
      "Epoch 44/100, Loss: 0.0001, LR: 0.001\n",
      "Epoch 45/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 46/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 47/100, Loss: 0.0001, LR: 0.001\n",
      "Epoch 48/100, Loss: 0.0001, LR: 0.001\n",
      "Epoch 49/100, Loss: 0.0001, LR: 0.001\n",
      "Epoch 50/100, Loss: 0.0001, LR: 0.001\n",
      "Epoch 51/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 52/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 53/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 54/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 55/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 56/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 57/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 58/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 59/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 60/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 61/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 62/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 63/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 64/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 65/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 66/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 67/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 68/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 69/100, Loss: 0.0000, LR: 0.001\n",
      "Epoch 70/100, Loss: 0.0001, LR: 0.0005\n",
      "Epoch 71/100, Loss: 0.0002, LR: 0.0005\n",
      "Epoch 72/100, Loss: 0.0000, LR: 0.0005\n",
      "Epoch 73/100, Loss: 0.0002, LR: 0.0005\n",
      "Epoch 74/100, Loss: 0.0000, LR: 0.0005\n",
      "Epoch 75/100, Loss: 0.0001, LR: 0.0005\n",
      "Epoch 76/100, Loss: 0.0000, LR: 0.0005\n",
      "Epoch 77/100, Loss: 0.0001, LR: 0.0005\n",
      "Epoch 78/100, Loss: 0.0000, LR: 0.0005\n",
      "Epoch 79/100, Loss: 0.0001, LR: 0.0005\n",
      "Epoch 80/100, Loss: 0.0000, LR: 0.0005\n",
      "Epoch 81/100, Loss: 0.0001, LR: 0.0005\n",
      "Epoch 82/100, Loss: 0.0000, LR: 0.0005\n",
      "Epoch 83/100, Loss: 0.0001, LR: 0.0005\n",
      "Epoch 84/100, Loss: 0.0000, LR: 0.0005\n",
      "Epoch 85/100, Loss: 0.0000, LR: 0.0005\n",
      "Epoch 86/100, Loss: 0.0000, LR: 0.0005\n",
      "Epoch 87/100, Loss: 0.0000, LR: 0.0005\n",
      "Epoch 88/100, Loss: 0.0000, LR: 0.0005\n",
      "Epoch 89/100, Loss: 0.0000, LR: 0.0005\n",
      "Epoch 90/100, Loss: 0.0000, LR: 0.0005\n",
      "Epoch 91/100, Loss: 0.0000, LR: 0.0005\n",
      "Epoch 92/100, Loss: 0.0000, LR: 0.0005\n",
      "Epoch 93/100, Loss: 0.0000, LR: 0.0005\n",
      "Epoch 94/100, Loss: 0.0000, LR: 0.0005\n",
      "Epoch 95/100, Loss: 0.0000, LR: 0.00025\n",
      "Epoch 96/100, Loss: 0.0000, LR: 0.00025\n",
      "Epoch 97/100, Loss: 0.0000, LR: 0.00025\n",
      "Epoch 98/100, Loss: 0.0000, LR: 0.00025\n",
      "Epoch 99/100, Loss: 0.0000, LR: 0.00025\n",
      "Epoch 100/100, Loss: 0.0000, LR: 0.00025\n",
      "Training completed in 0.10 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now we define the training loop\n",
    "# We define the loss function (Mean Squared Error) and the optimizer (Adam)\n",
    "# Then we iterate over the data in batches, perform forward pass, compute loss, backpropagate and update weights\n",
    "\n",
    "# Define Mean Squared Error loss (for regression)\n",
    "#loss_fn = nn.MSELoss()\n",
    "# Define Cross Entropy loss (for classification)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)  # Reduce LR by gamma after step_size epochs\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)  # Cosine annealing scheduler, gradually reduces learning rate, specify eta_min=X for minimum LR value (default 0)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)  # Reduce LR on plateau, reduce LR by factor of 0.5 if validation loss does not improve for 10 epochs\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Check output for a single training step\n",
    "optimizer.zero_grad()  # Reset gradients\n",
    "test_outputs = model(test_inputs)  # Forward pass (predict N)\n",
    "test_loss = loss_fn(test_outputs, test_targets)  # Compute loss\n",
    "test_loss.backward()  # Backpropagation\n",
    "optimizer.step()  # Update weights\n",
    "print(\"Model Output After One Training Step:\")\n",
    "print(model(test_inputs))\n",
    "\n",
    "\n",
    "start_time = time.time()  # Track how long training takes\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0  # Track total loss for the epoch\n",
    "\n",
    "    for inputs, targets in dataloader:  # Iterate over batches\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(inputs)  # Forward pass (predict N)\n",
    "        loss = loss_fn(outputs, targets)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        total_loss += loss.item()  # Keep track of loss\n",
    "\n",
    "    # Step the scheduler every epoch\n",
    "    #scheduler.step()\n",
    "    # Use for the ReduceLROnPlateau scheduler\n",
    "    scheduler.step(total_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}, LR: {scheduler.get_last_lr()[0]}\")  # Print progress\n",
    "\n",
    "print(f\"Training completed in {time.time() - start_time:.2f} seconds\")  # Print total time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True N: 12056, Predicted N: 32298\n",
      "True N: 30140, Predicted N: 69283\n",
      "True N: 44814, Predicted N: 99066\n",
      "True N: 137700, Predicted N: 56465\n",
      "True N: 108445, Predicted N: 135402\n",
      "True N: 142177, Predicted N: 150794\n",
      "True N: 100557, Predicted N: 159804\n",
      "True N: 19161, Predicted N: 160711\n",
      "True N: 64601, Predicted N: 123332\n",
      "True N: 30270, Predicted N: 82207\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now we evaluate the model on a new test dataset\n",
    "# This dataset was not seen by the model during training\n",
    "# We will use the trained model to predict the number N from the moduli\n",
    "# Generate new test dataset (never seen before)\n",
    "num_test_samples = 10  # Number of test examples\n",
    "\n",
    "test_samples = []\n",
    "for _ in range(num_test_samples):\n",
    "    N = random.randint(0, P - 1)  # New random integer N\n",
    "    normalized_remainders = [2*(N % p) / p - 1 for p in primes]  # Normalized moduli\n",
    "    target = (N - (P / 2)) / (P / 2)  # Normalized target\n",
    "    test_samples.append((torch.tensor(normalized_remainders, dtype=torch.float32), torch.tensor(target, dtype=torch.float32)))\n",
    "\n",
    "# Convert test samples into a PyTorch Dataset and DataLoader\n",
    "test_dataset = ModuloDataset(num_samples=num_test_samples, myprimes=primes)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Evaluate model on test data\n",
    "model.eval()  # Set model to evaluation mode\n",
    "predictions = []\n",
    "true_values = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations for efficiency\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)  # Get model predictions\n",
    "        predictions.extend(outputs.tolist())  # Store predicted values\n",
    "        true_values.extend(targets.tolist())  # Store true values\n",
    "\n",
    "# Convert back to original scale\n",
    "true_values = [(t * (P / 2)) + (P / 2) for t in true_values]\n",
    "predictions = [(p * (P / 2)) + (P / 2) for p in predictions]\n",
    "\n",
    "# Print first 10 predictions vs actual values\n",
    "for i in range(10):\n",
    "    print(f\"True N: {int(true_values[i])}, Predicted N: {int(predictions[i])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
