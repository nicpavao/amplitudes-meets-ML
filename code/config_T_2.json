{
    "dataset_parameters": {
      "primes_list": [11,13,17,23],
      "number_samples": 10000
    },
    "model_parameters": {
      "model": "Seq2SeqTransformer",
      "model_dimension": 128,
      "number_heads": 8,
      "number_encoder_layers": 4,
      "number_decoder_layers": 4,
      "dimension_feedforward": 512,
      "dropout_rate": 0.1,
      "source_vocab_size": 14,
      "target_vocab_size": 13,
      "positional_encoding_maximum_length": 500
    },
    "training_parameters": {
      "learning_rate": 1e-3,
      "batch_size": 32,
      "number_epochs": 100,
      "optimizer": "Adam"
    },
    "log_params": {
      "experiment_name": "experiment_001",
      "notes": "First experiment with Seq2SeqTransformer"
    },
    "special_tokens": {
      "PAD_TOKEN": 10,
      "SOS_TOKEN": 11,
      "EOS_TOKEN": 12,
      "SEP_TOKEN": 13
    }
  }
