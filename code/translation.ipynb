{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is my first transformer project\n",
    "# The idea is to create a transformer that can take as imput a list (N mod p1, N mod p2, ..., N mod pn)\n",
    "# where N is a number (integer, rational, etc) and p1,p2,...,pn are prime numbers\n",
    "# and returns the number N\n",
    "\n",
    "# %% \n",
    "# Let us start by importing the necessary libraries\n",
    "import torch  # Main framework for defining and training the transformer\n",
    "import torch.nn as nn  # Neural network module\n",
    "import torch.optim as optim  # Optimization functions\n",
    "import numpy as np  # For numerical operations\n",
    "import random  # For generating random numbers\n",
    "import itertools  # (Optional) For generating structured datasets\n",
    "import math  # For mathematical operations\n",
    "\n",
    "import matplotlib.pyplot as plt  # (Optional) For visualization\n",
    "from torch.utils.data import Dataset, DataLoader  # To handle training data efficiently\n",
    "\n",
    "import time # For timing the training process\n",
    "\n",
    "import json # For saving and loading the model\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence # For padding sequences to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Define the device to use for training\n",
    "#OLD: device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")          # Apple-silicon GPU\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")         # NVIDIA / AMD GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration:\n",
      "{'dataset_parameters': {'primes_list': [11, 13, 17, 23], 'number_samples': 10000}, 'model_parameters': {'model': 'Seq2SeqTransformer', 'model_dimension': 128, 'number_heads': 8, 'number_encoder_layers': 4, 'number_decoder_layers': 4, 'dimension_feedforward': 512, 'dropout_rate': 0.1, 'source_vocab_size': 14, 'target_vocab_size': 13, 'positional_encoding_maximum_length': 500}, 'training_parameters': {'learning_rate': 0.001, 'batch_size': 32, 'number_epochs': 100, 'optimizer': 'Adam'}, 'log_params': {'experiment_name': 'experiment_001', 'notes': 'First experiment with Seq2SeqTransformer'}, 'special_tokens': {'PAD_TOKEN': 10, 'SOS_TOKEN': 11, 'EOS_TOKEN': 12, 'SEP_TOKEN': 13}}\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from a JSON file\n",
    "with open(\"config_T_2.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Access dataset parameters:\n",
    "primes_list = config[\"dataset_parameters\"][\"primes_list\"]  # List of prime numbers\n",
    "number_samples = config[\"dataset_parameters\"][\"number_samples\"]  # Number of samples to generate\n",
    "\n",
    "# Access model parameters:\n",
    "model_dimension = config[\"model_parameters\"][\"model_dimension\"]\n",
    "number_heads = config[\"model_parameters\"][\"number_heads\"]\n",
    "number_encoder_layers = config[\"model_parameters\"][\"number_encoder_layers\"]\n",
    "number_decoder_layers = config[\"model_parameters\"][\"number_decoder_layers\"]\n",
    "dimension_feedforward = config[\"model_parameters\"][\"dimension_feedforward\"]\n",
    "dropout_rate = config[\"model_parameters\"][\"dropout_rate\"]\n",
    "max_length = config[\"model_parameters\"][\"positional_encoding_maximum_length\"]  # Maximum sequence length for positional encoding\n",
    "# source and target vocabulary\n",
    "# For instance, suppose our source vocabulary (mod values) is 0..11 and target vocabulary (digits) is 0..9 plus a special token.\n",
    "src_vocab_size = config[\"model_parameters\"][\"source_vocab_size\"]   # digits 0-9 plus 4 special tokens (SOS, EOS, SEP, PAD)\n",
    "tgt_vocab_size = config[\"model_parameters\"][\"target_vocab_size\"]   # digits 0-9 plus 2 special tokens (SOS, EOS)\n",
    "\n",
    "learning_rate = config[\"training_parameters\"][\"learning_rate\"]\n",
    "batch_size = config[\"training_parameters\"][\"batch_size\"]\n",
    "number_epochs = config[\"training_parameters\"][\"number_epochs\"]\n",
    "\n",
    "print(\"Loaded configuration:\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"MPS available:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from dataset: (tensor([11,  1,  0, 13,  4, 13,  1,  3, 13,  0, 12]), tensor([11,  1,  2,  6,  2,  7, 12]))\n"
     ]
    }
   ],
   "source": [
    "# Define special tokens\n",
    "SOS_TOKEN = config[\"special_tokens\"][\"SOS_TOKEN\"]   # start-of-sequence for target\n",
    "EOS_TOKEN = config[\"special_tokens\"][\"EOS_TOKEN\"]   # end-of-sequence for target\n",
    "SEP_TOKEN = config[\"special_tokens\"][\"SEP_TOKEN\"]   # separator token for input moduli\n",
    "\n",
    "def tokenize_moduli(N, primes):\n",
    "    tokens = [SOS_TOKEN] # Start with the SOS token\n",
    "    for p in primes:\n",
    "        remainder = N % p\n",
    "        # Convert the remainder into its constituent digits\n",
    "        tokens.extend([int(d) for d in str(remainder)])\n",
    "        # Append a separator token after each remainder\n",
    "        tokens.append(SEP_TOKEN)\n",
    "    # Remove the final separator since it's not needed\n",
    "    if tokens:\n",
    "        tokens = tokens[:-1]\n",
    "    # Append the EOS token at the end\n",
    "    tokens.append(EOS_TOKEN)\n",
    "    return tokens\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, num_samples=number_samples, primes=primes_list):\n",
    "        self.primes = primes\n",
    "        # Calculate the product of primes for range of N\n",
    "        self.P = 1\n",
    "        for p in primes:\n",
    "            self.P *= p\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            # Generate a random integer N in [0, P)\n",
    "            N = torch.randint(0, self.P, (1,)).item()\n",
    "            # Tokenize the input: each remainder becomes a sequence of digits with separators SEP\n",
    "            input_tokens = tokenize_moduli(N, primes)\n",
    "            # Prepare the target: add <SOS> at the beginning and <EOS> at the end\n",
    "            output_tokens = [SOS_TOKEN] + [int(d) for d in str(N)] + [EOS_TOKEN]\n",
    "            self.samples.append((torch.tensor(input_tokens, dtype=torch.long),\n",
    "                                 torch.tensor(output_tokens, dtype=torch.long)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "# Quick test of the updated dataset:\n",
    "dataset = TranslationDataset(num_samples=number_samples)\n",
    "print(\"Sample from dataset:\", dataset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014\n",
      "[2, 0, 11, 2]\n",
      "[11, 2, 13, 0, 13, 1, 1, 13, 2, 12]\n"
     ]
    }
   ],
   "source": [
    "#simple test to check the tokenization function\n",
    "testnum = 1014  # Get a random sample from the dataset\n",
    "print(testnum)\n",
    "print([testnum % p for p in primes_list])\n",
    "print(tokenize_moduli(testnum, primes_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PAD_TOKEN = config[\"special_tokens\"][\"PAD_TOKEN\"]  # Define a PAD token index (adjust your vocab sizes accordingly)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Each batch element is a tuple: (src, tgt)\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    # Pad the sequences so that all sequences in the batch have equal length\n",
    "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_TOKEN)\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_TOKEN)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "# Create a DataLoader using the collate function:\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional encoding + masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)  # Create a (max_len, d_model) matrix.\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # Shape: (max_len, 1) with positions 0,1,2,...\n",
    "        # Compute a scaling factor for each even dimension.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # For even indices: use sine; for odd indices: use cosine.\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # Shape becomes (max_len, 1, d_model) for easy broadcasting.\n",
    "        self.register_buffer('pe', pe)  # Register as a buffer so it’s part of the module but not a parameter.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_len, batch_size, d_model)\n",
    "        x = x + self.pe[:x.size(0)]  # Add positional encoding to each token embedding.\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original token matrix:\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "\n",
      "Positional encodings (first 4 positions):\n",
      "tensor([[[0., 1., 0., 1.]]])\n",
      "\n",
      "Token matrix after adding positional encoding:\n",
      "tensor([[[0., 1., 0., 1.],\n",
      "         [0., 1., 0., 1.],\n",
      "         [0., 1., 0., 1.],\n",
      "         [0., 1., 0., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# Test parameters\n",
    "d_model = 4      # Dimensionality of embeddings/positional encodings\n",
    "seq_len = 1      # Sequence length\n",
    "batch_size = 4   # Batch size\n",
    "\n",
    "# Create a dummy token matrix (for example, all zeros)\n",
    "dummy_tokens = torch.zeros(seq_len, batch_size, d_model)\n",
    "print(\"Original token matrix:\")\n",
    "print(dummy_tokens)\n",
    "\n",
    "# Instantiate PositionalEncoding with no dropout for clarity\n",
    "pos_enc = PositionalEncoding(d_model, dropout=0.0, max_len=10)\n",
    "\n",
    "print(\"\\nPositional encodings (first 4 positions):\")\n",
    "# The positional encoding matrix has shape (max_len, 1, d_model)\n",
    "# We'll print the first 4 positions, which correspond to our sequence length.\n",
    "print(pos_enc.pe[:seq_len])\n",
    "\n",
    "# Add positional encoding to the dummy tokens\n",
    "tokens_with_pe = pos_enc(dummy_tokens)\n",
    "print(\"\\nToken matrix after adding positional encoding:\")\n",
    "print(tokens_with_pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Mask:\n",
      " tensor([[False,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True],\n",
      "        [False, False, False,  True,  True],\n",
      "        [False, False, False, False,  True],\n",
      "        [False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "# OLD float mask\n",
    "#def generate_square_subsequent_mask(sz):\n",
    "#    # Create an upper-triangular matrix filled with ones\n",
    "#    mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "#    # Replace 1's with -infinity and 0's with 0.0 so that the softmax later ignores the future positions.\n",
    "#    mask = mask.masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))\n",
    "#    return mask\n",
    "\n",
    "# NEW boolean mask\n",
    "def generate_square_subsequent_mask(sz, device=None):\n",
    "    \"\"\"\n",
    "    Returns a boolean matrix of shape (sz, sz) where\n",
    "    `True`  = block attention (upper-triangle, i.e. future positions)\n",
    "    `False` = allow attention (diagonal & lower-triangle)\n",
    "    \"\"\"\n",
    "    return torch.triu(\n",
    "        torch.ones(sz, sz, dtype=torch.bool, device=device),\n",
    "        diagonal=1\n",
    "    )\n",
    "\n",
    "# Example usage:\n",
    "tgt_seq_len = 5  # suppose our target sequence length is 5\n",
    "tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "print(\"Target Mask:\\n\", tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq Transformer model\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=model_dimension, nhead=number_heads,\n",
    "                 num_encoder_layers=number_encoder_layers, num_decoder_layers=number_decoder_layers, dim_feedforward=dimension_feedforward, dropout=dropout_rate):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        # Embedding layers for source (moduli) and target (digits).\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        # Positional encodings for source and target.\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=max_length)\n",
    "        self.pos_decoder = PositionalEncoding(d_model, dropout, max_len=max_length)\n",
    "        # Transformer module from PyTorch.\n",
    "        self.transformer = nn.Transformer(d_model, nhead,\n",
    "                                          num_encoder_layers, num_decoder_layers,\n",
    "                                          dim_feedforward, dropout)\n",
    "        # Final linear layer maps transformer output to target vocabulary logits.\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        # Expect src and tgt shapes: (batch_size, seq_len)\n",
    "        # Transpose to shape: (seq_len, batch_size) as required by the transformer.\n",
    "        src = src.transpose(0, 1)\n",
    "        tgt = tgt.transpose(0, 1)\n",
    "        # Obtain token embeddings and apply scaling.\n",
    "        src_emb = self.src_embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        # Add positional encodings.\n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "        tgt_emb = self.pos_decoder(tgt_emb)\n",
    "        # Forward pass through the transformer.\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask=src_mask, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        # Project transformer outputs to logits and transpose back to (batch_size, seq_len, vocab_size).\n",
    "        logits = self.fc_out(outs)\n",
    "        return logits.transpose(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqTransformer(\n",
      "  (src_embedding): Embedding(14, 128)\n",
      "  (tgt_embedding): Embedding(13, 128)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (pos_decoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=128, out_features=13, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paolo/Desktop/Projects/MLwithNic/amplitudes-meets-ML/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instantiate the model.\n",
    "model = Seq2SeqTransformer(src_vocab_size, tgt_vocab_size, d_model=model_dimension, nhead=number_heads,\n",
    "                           num_encoder_layers=number_encoder_layers, num_decoder_layers=number_decoder_layers, dim_feedforward=dimension_feedforward, dropout=dropout_rate)\n",
    "# Move the model to the appropriate device (GPU or CPU).\n",
    "model.to(device)\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First examine sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11,  3, 13,  2, 13,  0, 13,  9, 12, 10, 10],\n",
      "        [11,  2, 13,  8, 13,  1,  2, 13,  3, 12, 10],\n",
      "        [11,  1, 13,  1,  1, 13,  2, 13,  1,  9, 12],\n",
      "        [11,  8, 13,  1, 13,  7, 13,  2,  1, 12, 10]], device='mps:0')\n",
      "tensor([[11,  1,  5,  8,  1,  0, 12],\n",
      "        [11,  8,  8,  3,  5, 12, 10],\n",
      "        [11,  3,  7,  5,  5,  5, 12],\n",
      "        [11,  1,  3,  1,  3,  1, 12]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "#define a simple test dataset\n",
    "batch_size = 4\n",
    "primes = primes_list\n",
    "sample_ds = TranslationDataset(num_samples=batch_size, primes=primes)\n",
    "src, tgt = zip(*sample_ds)                 # list of tensors\n",
    "src = pad_sequence(src, batch_first=True, padding_value=PAD_TOKEN)\n",
    "tgt = pad_sequence(tgt, batch_first=True, padding_value=PAD_TOKEN)\n",
    "src, tgt = src.to(device), tgt.to(device)\n",
    "print(src)\n",
    "print(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target input (tgt_in): tensor([[11,  1,  5,  8,  1,  0],\n",
      "        [11,  8,  8,  3,  5, 12],\n",
      "        [11,  3,  7,  5,  5,  5],\n",
      "        [11,  1,  3,  1,  3,  1]], device='mps:0')\n",
      "Target labels (tgt_lab): tensor([[ 1,  5,  8,  1,  0, 12],\n",
      "        [ 8,  8,  3,  5, 12, 10],\n",
      "        [ 3,  7,  5,  5,  5, 12],\n",
      "        [ 1,  3,  1,  3,  1, 12]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Teacher-forcing split\n",
    "# i.e. return the target sequence without the last token and the target sequence without the first token\n",
    "# these two cases are used as part of the transformer architecture\n",
    "tgt_in  = tgt[:, :-1]\n",
    "tgt_lab = tgt[:, 1:]\n",
    "print(\"Target input (tgt_in):\", tgt_in)\n",
    "print(\"Target labels (tgt_lab):\", tgt_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then generate and print target mask and source + target padding masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source key padding mask (src_kpm): tensor([[False, False, False, False, False, False, False, False, False,  True,\n",
      "          True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "          True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "          True]], device='mps:0')\n",
      "Target key padding mask (tgt_kpm): tensor([[False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False]], device='mps:0')\n",
      "Target mask (tgt_mask): tensor([[False,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Generate masks for the source and target sequences\n",
    "# Note masks will only be True if there is a padding token in the sequence\n",
    "tgt_mask = generate_square_subsequent_mask(tgt_in.size(1),device=src.device)\n",
    "src_kpm = (src == PAD_TOKEN)\n",
    "tgt_kpm = (tgt_in == PAD_TOKEN)\n",
    "print(\"Source key padding mask (src_kpm):\", src_kpm)\n",
    "print(\"Target key padding mask (tgt_kpm):\", tgt_kpm)\n",
    "print(\"Target mask (tgt_mask):\", tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then forward pass through the untrained model and check this behaves as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ forward pass shape OK\n"
     ]
    }
   ],
   "source": [
    "# Forward pass through the model, check no errors\n",
    "logits = model(\n",
    "    src, tgt_in,\n",
    "    src_mask=None,\n",
    "    tgt_mask=tgt_mask,\n",
    "    src_key_padding_mask=src_kpm,\n",
    "    tgt_key_padding_mask=tgt_kpm,\n",
    "    memory_key_padding_mask=src_kpm\n",
    ")\n",
    "\n",
    "assert logits.shape == (batch_size, tgt_in.size(1), tgt_vocab_size)\n",
    "print(\"✓ forward pass shape OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute loss and check this behaves as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ backward gradient computed, loss = 2.519232749938965\n"
     ]
    }
   ],
   "source": [
    "# Compute the loss using CrossEntropyLoss\n",
    "# check everything works as expected\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_lab.reshape(-1))\n",
    "\n",
    "loss.backward()        # should succeed without NaNs/Infs\n",
    "print(\"✓ backward gradient computed, loss =\", float(loss))\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 1857037\n"
     ]
    }
   ],
   "source": [
    "# count trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters:\", total_params)\n",
    "# ≈ 420 k for 2-layer enc/dec, d_model 64, FF 128, vocab ≤ 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the attention weights in the first encoder layer are zero for padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights onto PAD key (col-0):  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='mps:0')\n",
      "Weights onto real key (col-1): tensor([0.1389, 0.1373, 0.1389, 0.1388, 0.2778, 0.2778, 0.2776, 0.2778, 0.1389,\n",
      "        0.2778], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# 1) Build a two-sample batch and inject PAD at position 0 of sample 0\n",
    "ds = TranslationDataset(num_samples=2)\n",
    "src_batch, _ = zip(*ds)\n",
    "src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_TOKEN)\n",
    "src_batch[0, 0] = PAD_TOKEN                         # <- forced PAD\n",
    "src_batch = src_batch.to(device)                  # move to device\n",
    "\n",
    "# 2) Grab that first encoder self-attention module\n",
    "mha = model.transformer.encoder.layers[0].self_attn\n",
    "\n",
    "# 3) Prepare the same embeddings the encoder would see\n",
    "with torch.no_grad():\n",
    "    x = model.pos_encoder(                          # add positions\n",
    "            model.src_embedding(src_batch.T) *      # embed IDs ➜ 64-D\n",
    "            math.sqrt(model.d_model)                # √d scaling\n",
    "        )                                           # shape (seq, batch, 64)\n",
    "\n",
    "    # 4) Run *just* the attention, asking for weights\n",
    "    attn_out, attn_w = mha(                         # attn_w shape: (batch, heads, seq, seq)\n",
    "        x, x, x,\n",
    "        need_weights=True,\n",
    "        average_attn_weights=False,\n",
    "        key_padding_mask=(src_batch == PAD_TOKEN)   # boolean PAD mask\n",
    "    )\n",
    "\n",
    "# 5) Average over heads, then print two key columns\n",
    "W = attn_w.mean(1)              # (batch, seq, seq)\n",
    "\n",
    "# here we look at the attention weights for the first sample\n",
    "# first we look at columnn 0, ie the attention weights referring to the PAD token\n",
    "# then we look at column 1, ie the attention weights referring to the real key\n",
    "print(\"Weights onto PAD key (col-0): \",  W[0, :, 0])\n",
    "print(\"Weights onto real key (col-1):\",  W[0, :, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run a test training with a single sample to make sure we can overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   0 | loss 2.6668\n",
      "step  50 | loss 0.1900\n",
      "step 100 | loss 0.1483\n",
      "step 150 | loss 0.0311\n",
      "step 200 | loss 0.6515\n",
      "step 250 | loss 0.0505\n",
      "step 299 | loss 0.0676\n",
      "step 300 | loss 0.0548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (src_embedding): Embedding(14, 128)\n",
       "  (tgt_embedding): Embedding(13, 128)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pos_decoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=128, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()                              # ensure dropout is ON (helps test realism)\n",
    "\n",
    "# 0. For reproducibility (optional)\n",
    "torch.manual_seed(0);  np.random.seed(0);  random.seed(0)\n",
    "\n",
    "# 1. Grab *one* random (src, tgt) pair\n",
    "sample_src, sample_tgt = TranslationDataset(num_samples=1)[0]\n",
    "sample_src = sample_src.unsqueeze(0)       # shape (1, src_len)\n",
    "sample_tgt = sample_tgt.unsqueeze(0)       # shape (1, tgt_len)\n",
    "sample_src, sample_tgt = sample_src.to(device), sample_tgt.to(device)  # move to device\n",
    "\n",
    "# 2. Optimiser & loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion  = torch.nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# 3. Training loop\n",
    "for step in range(301):\n",
    "    # --- teacher-forcing split --------------------------------------------\n",
    "    tgt_in  = sample_tgt[:, :-1]           # BOS … last-1\n",
    "    tgt_lab = sample_tgt[:, 1:]            # next token (labels)\n",
    "\n",
    "    # --- boolean masks ----------------------------------------------------\n",
    "    src_kpm = (sample_src == PAD_TOKEN)               # shape (1, src_len)\n",
    "    tgt_kpm = (tgt_in    == PAD_TOKEN)               # shape (1, tgt_len-1)\n",
    "    tgt_mask = generate_square_subsequent_mask(\n",
    "                  tgt_in.size(1), device=sample_src.device)\n",
    "\n",
    "    # --- forward ----------------------------------------------------------\n",
    "    logits = model(\n",
    "        sample_src, tgt_in,\n",
    "        src_mask=None,\n",
    "        tgt_mask=tgt_mask,\n",
    "        src_key_padding_mask=src_kpm,\n",
    "        tgt_key_padding_mask=tgt_kpm,\n",
    "        memory_key_padding_mask=src_kpm\n",
    "    )\n",
    "\n",
    "    loss = criterion(\n",
    "        logits.reshape(-1, logits.size(-1)),         # (tokens ×  vocab)\n",
    "        tgt_lab.reshape(-1)                          # (tokens)\n",
    "    )\n",
    "\n",
    "    # --- backward & step --------------------------------------------------\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # --- monitor ----------------------------------------------------------\n",
    "    if step % 50 == 0 or step == 299:\n",
    "        print(f\"step {step:3d} | loss {loss.item():.4f}\")\n",
    "\n",
    "model.eval()                               # back to eval mode afterwards\n",
    "# -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paolo/Desktop/Projects/MLwithNic/amplitudes-meets-ML/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100 | avg tok-loss 2.0884\n",
      "Epoch   2/100 | avg tok-loss 1.9896\n",
      "Epoch   3/100 | avg tok-loss 1.9637\n",
      "Epoch   4/100 | avg tok-loss 1.9499\n",
      "Epoch   5/100 | avg tok-loss 1.9476\n",
      "Epoch   6/100 | avg tok-loss 1.9351\n",
      "Epoch   7/100 | avg tok-loss 1.9280\n",
      "Epoch   8/100 | avg tok-loss 1.9214\n",
      "Epoch   9/100 | avg tok-loss 1.9179\n",
      "Epoch  10/100 | avg tok-loss 2.0375\n",
      "  sample decode → \n",
      "Epoch  11/100 | avg tok-loss 2.3518\n",
      "Epoch  12/100 | avg tok-loss 2.3514\n",
      "Epoch  13/100 | avg tok-loss 2.3510\n",
      "Epoch  14/100 | avg tok-loss 2.3564\n",
      "Epoch  15/100 | avg tok-loss 2.0761\n",
      "Epoch  16/100 | avg tok-loss 1.9927\n",
      "Epoch  17/100 | avg tok-loss 1.9772\n",
      "Epoch  18/100 | avg tok-loss 1.9690\n",
      "Epoch  19/100 | avg tok-loss 1.9692\n",
      "Epoch  20/100 | avg tok-loss 2.0897\n",
      "  sample decode → \n",
      "Epoch  21/100 | avg tok-loss 2.2697\n",
      "Epoch  22/100 | avg tok-loss 2.2477\n",
      "Epoch  23/100 | avg tok-loss 2.2027\n",
      "Epoch  24/100 | avg tok-loss 2.1843\n",
      "Epoch  25/100 | avg tok-loss 2.1742\n",
      "Epoch  26/100 | avg tok-loss 2.1696\n",
      "Epoch  27/100 | avg tok-loss 2.1591\n",
      "Epoch  28/100 | avg tok-loss 2.1557\n",
      "Epoch  29/100 | avg tok-loss 2.1529\n",
      "Epoch  30/100 | avg tok-loss 2.1496\n",
      "  sample decode → 2222222222222222222\n",
      "Epoch  31/100 | avg tok-loss 2.1376\n",
      "Epoch  32/100 | avg tok-loss 2.1319\n",
      "Epoch  33/100 | avg tok-loss 2.1336\n",
      "Epoch  34/100 | avg tok-loss 2.1275\n",
      "Epoch  35/100 | avg tok-loss 2.1234\n",
      "Epoch  36/100 | avg tok-loss 2.1224\n",
      "Epoch  37/100 | avg tok-loss 2.1187\n",
      "Epoch  38/100 | avg tok-loss 2.1172\n",
      "Epoch  39/100 | avg tok-loss 2.1151\n",
      "Epoch  40/100 | avg tok-loss 2.1120\n",
      "  sample decode → 1111111111111111111\n",
      "Epoch  41/100 | avg tok-loss 2.1117\n",
      "Epoch  42/100 | avg tok-loss 2.1066\n",
      "Epoch  43/100 | avg tok-loss 2.1034\n",
      "Epoch  44/100 | avg tok-loss 2.1015\n",
      "Epoch  45/100 | avg tok-loss 2.1008\n",
      "Epoch  46/100 | avg tok-loss 2.0960\n",
      "Epoch  47/100 | avg tok-loss 2.0924\n",
      "Epoch  48/100 | avg tok-loss 2.0890\n",
      "Epoch  49/100 | avg tok-loss 2.0882\n",
      "Epoch  50/100 | avg tok-loss 2.0852\n",
      "  sample decode → 4444444444444444444\n",
      "Epoch  51/100 | avg tok-loss 2.0847\n",
      "Epoch  52/100 | avg tok-loss 2.0808\n",
      "Epoch  53/100 | avg tok-loss 2.0796\n",
      "Epoch  54/100 | avg tok-loss 2.0776\n",
      "Epoch  55/100 | avg tok-loss 2.0790\n",
      "Epoch  56/100 | avg tok-loss 2.0714\n",
      "Epoch  57/100 | avg tok-loss 2.0719\n",
      "Epoch  58/100 | avg tok-loss 2.0709\n",
      "Epoch  59/100 | avg tok-loss 2.0690\n",
      "Epoch  60/100 | avg tok-loss 2.0672\n",
      "  sample decode → 4444444444444444444\n",
      "Epoch  61/100 | avg tok-loss 2.0550\n",
      "Epoch  62/100 | avg tok-loss 2.0536\n",
      "Epoch  63/100 | avg tok-loss 2.0524\n",
      "Epoch  64/100 | avg tok-loss 2.0529\n",
      "Epoch  65/100 | avg tok-loss 2.0487\n",
      "Epoch  66/100 | avg tok-loss 2.0486\n",
      "Epoch  67/100 | avg tok-loss 2.0470\n",
      "Epoch  68/100 | avg tok-loss 2.0454\n",
      "Epoch  69/100 | avg tok-loss 2.0441\n",
      "Epoch  70/100 | avg tok-loss 2.0440\n",
      "  sample decode → 4444444444444444444\n",
      "Epoch  71/100 | avg tok-loss 2.0429\n",
      "Epoch  72/100 | avg tok-loss 2.0424\n",
      "Epoch  73/100 | avg tok-loss 2.0432\n",
      "Epoch  74/100 | avg tok-loss 2.0398\n",
      "Epoch  75/100 | avg tok-loss 2.0387\n",
      "Epoch  76/100 | avg tok-loss 2.0397\n",
      "Epoch  77/100 | avg tok-loss 2.0386\n",
      "Epoch  78/100 | avg tok-loss 2.0365\n",
      "Epoch  79/100 | avg tok-loss 2.0316\n",
      "Epoch  80/100 | avg tok-loss 2.0337\n",
      "  sample decode → 4444444444444444444\n",
      "Epoch  81/100 | avg tok-loss 2.0307\n",
      "Epoch  82/100 | avg tok-loss 2.0286\n",
      "Epoch  83/100 | avg tok-loss 2.0292\n",
      "Epoch  84/100 | avg tok-loss 2.0262\n",
      "Epoch  85/100 | avg tok-loss 2.0270\n",
      "Epoch  86/100 | avg tok-loss 2.0266\n",
      "Epoch  87/100 | avg tok-loss 2.0265\n",
      "Epoch  88/100 | avg tok-loss 2.0228\n",
      "Epoch  89/100 | avg tok-loss 2.0246\n",
      "Epoch  90/100 | avg tok-loss 2.0231\n",
      "  sample decode → 4444444444444444444\n",
      "Epoch  91/100 | avg tok-loss 2.0141\n",
      "Epoch  92/100 | avg tok-loss 2.0114\n",
      "Epoch  93/100 | avg tok-loss 2.0130\n",
      "Epoch  94/100 | avg tok-loss 2.0115\n",
      "Epoch  95/100 | avg tok-loss 2.0116\n",
      "Epoch  96/100 | avg tok-loss 2.0105\n",
      "Epoch  97/100 | avg tok-loss 2.0106\n",
      "Epoch  98/100 | avg tok-loss 2.0093\n",
      "Epoch  99/100 | avg tok-loss 2.0080\n",
      "Epoch 100/100 | avg tok-loss 2.0093\n",
      "  sample decode → 1111111111111111111\n"
     ]
    }
   ],
   "source": [
    "# ── reproducibility (optional)\n",
    "torch.manual_seed(0); np.random.seed(0); random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ── (re-)instantiate model & move to device\n",
    "model = Seq2SeqTransformer(src_vocab_size, tgt_vocab_size,\n",
    "                           d_model=model_dimension, nhead=number_heads,\n",
    "                           num_encoder_layers=number_encoder_layers,\n",
    "                           num_decoder_layers=number_decoder_layers,\n",
    "                           dim_feedforward=dimension_feedforward,\n",
    "                           dropout=dropout_rate).to(device)\n",
    "\n",
    "# ── criterion & optimiser\n",
    "criterion  = torch.nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "optimizer   = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler   = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "\n",
    "# ---------------- 1. TRAINING LOOP ----------------\n",
    "for epoch in range(1, number_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss, n_tokens = 0.0, 0\n",
    "\n",
    "    for src, tgt in data_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        # Teacher forcing split\n",
    "        tgt_in  = tgt[:, :-1]          # BOS … last-1\n",
    "        tgt_lab = tgt[:, 1:]           # next token\n",
    "\n",
    "        # Boolean masks\n",
    "        src_kpm = (src == PAD_TOKEN)                   # (batch, src_len)\n",
    "        tgt_kpm = (tgt_in == PAD_TOKEN)                # (batch, tgt_len-1)\n",
    "        tgt_mask = generate_square_subsequent_mask(\n",
    "                       tgt_in.size(1), device=device)  # (tgt_len-1, tgt_len-1)\n",
    "\n",
    "        # Forward\n",
    "        logits = model(src, tgt_in,\n",
    "                       src_mask=None,\n",
    "                       tgt_mask=tgt_mask,\n",
    "                       src_key_padding_mask=src_kpm,\n",
    "                       tgt_key_padding_mask=tgt_kpm,\n",
    "                       memory_key_padding_mask=src_kpm)\n",
    "\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)),\n",
    "                         tgt_lab.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # optional\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * tgt_lab.numel()\n",
    "        n_tokens     += tgt_lab.numel()\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = running_loss / n_tokens\n",
    "    print(f\"Epoch {epoch:3d}/{number_epochs} | avg tok-loss {avg_loss:.4f}\")\n",
    "\n",
    "    # ---- quick sanity decode every 10 epochs ----\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            sample_src, _ = dataset[random.randrange(len(dataset))]\n",
    "            sample_src = sample_src.unsqueeze(0).to(device)\n",
    "            src_kpm = (sample_src == PAD_TOKEN)\n",
    "\n",
    "            # start with BOS\n",
    "            generated = [SOS_TOKEN]\n",
    "            for _ in range(20):                          # max 20 digits\n",
    "                tgt_in  = torch.tensor([generated], device=device)\n",
    "                tgt_mask = generate_square_subsequent_mask(\n",
    "                               tgt_in.size(1), device=device)\n",
    "                logits = model(sample_src, tgt_in,\n",
    "                               src_mask=None, tgt_mask=tgt_mask,\n",
    "                               src_key_padding_mask=src_kpm,\n",
    "                               tgt_key_padding_mask=(tgt_in == PAD_TOKEN),\n",
    "                               memory_key_padding_mask=src_kpm)\n",
    "                next_tok = logits[0, -1].argmax(-1).item()\n",
    "                generated.append(next_tok)\n",
    "                if next_tok == EOS_TOKEN:\n",
    "                    break\n",
    "\n",
    "            # remove BOS/EOS and print digits\n",
    "            digits = [str(t) for t in generated[1:-1]]\n",
    "            print(\"  sample decode →\", \"\".join(digits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to convert a list of moduli into an appropriate PyTorch source tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moduli_to_tensor(mod_list, device, add_batch_dim=True):\n",
    "    \"\"\"\n",
    "    mod_list:  [r1, r2, …, rn]  in the *same prime order* you used for training.\n",
    "    Returns:   tensor(shape=(1, src_len)) on the chosen device.\n",
    "    \"\"\"\n",
    "    tokens = [SOS_TOKEN]\n",
    "    for r in mod_list:\n",
    "        tokens.extend([int(d) for d in str(r)])   # decimal digits\n",
    "        tokens.append(SEP_TOKEN)\n",
    "    tokens[-1] = EOS_TOKEN                        # replace last SEP with EOS\n",
    "    t = torch.tensor(tokens, dtype=torch.long, device=device)\n",
    "    return t.unsqueeze(0) if add_batch_dim else t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a list of moduli and pass through the model to predict integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test moduli: [2, 0, 0, 2]\n",
      "  sample decode → 1080\n"
     ]
    }
   ],
   "source": [
    "test_moduli = [2,0,0,2]  # Example moduli for testing\n",
    "print(\"Test moduli:\", test_moduli)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_src = moduli_to_tensor(test_moduli, device)\n",
    "    src_kpm = (sample_src == PAD_TOKEN)\n",
    "\n",
    "    # start with SOS\n",
    "    generated = [SOS_TOKEN]\n",
    "    for _ in range(20):                          # max 20 digits\n",
    "        tgt_in  = torch.tensor([generated], device=device)\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_in.size(1), device=device)\n",
    "        logits = model(sample_src, tgt_in,\n",
    "                        src_mask=None, tgt_mask=tgt_mask,\n",
    "                        src_key_padding_mask=src_kpm,\n",
    "                        tgt_key_padding_mask=(tgt_in == PAD_TOKEN),\n",
    "                        memory_key_padding_mask=src_kpm)\n",
    "        next_tok = logits[0, -1].argmax(-1).item()\n",
    "        generated.append(next_tok)\n",
    "        if next_tok == EOS_TOKEN:\n",
    "            break\n",
    "\n",
    "    # remove BOS/EOS and print digits\n",
    "    digits = [str(t) for t in generated[1:-1]]\n",
    "    print(\"  sample decode →\", \"\".join(digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print([1080 % p for p in primes_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55913\n"
     ]
    }
   ],
   "source": [
    "print(11*13*17*23)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
