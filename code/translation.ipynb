{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is my first transformer project\n",
    "# The idea is to create a transformer that can take as imput a list (N mod p1, N mod p2, ..., N mod pn)\n",
    "# where N is a number (integer, rational, etc) and p1,p2,...,pn are prime numbers\n",
    "# and returns the number N\n",
    "\n",
    "# %% \n",
    "# Let us start by importing the necessary libraries\n",
    "import torch  # Main framework for defining and training the transformer\n",
    "import torch.nn as nn  # Neural network module\n",
    "import torch.optim as optim  # Optimization functions\n",
    "import numpy as np  # For numerical operations\n",
    "import random  # For generating random numbers\n",
    "import itertools  # (Optional) For generating structured datasets\n",
    "import math  # For mathematical operations\n",
    "\n",
    "import matplotlib.pyplot as plt  # (Optional) For visualization\n",
    "from torch.utils.data import Dataset, DataLoader  # To handle training data efficiently\n",
    "\n",
    "import time # For timing the training process\n",
    "\n",
    "import json # For saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration:\n",
      "{'model_params': {'model': 'MLP', 'input_dim': 5, 'hidden_dim': 512, 'output_dim': 1}, 'training_params': {'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 100, 'optimizer': 'Adam'}, 'log_params': {'experiment_name': 'experiment_001', 'notes': 'Baseline experiment with MLP'}}\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from a JSON file\n",
    "with open(\"config1.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Access parameters like:\n",
    "input_dim = config[\"model_params\"][\"input_dim\"]\n",
    "hidden_dim = config[\"model_params\"][\"hidden_dim\"]\n",
    "output_dim = config[\"model_params\"][\"output_dim\"]\n",
    "\n",
    "learning_rate = config[\"training_params\"][\"learning_rate\"]\n",
    "batch_size = config[\"training_params\"][\"batch_size\"]\n",
    "num_epochs = config[\"training_params\"][\"num_epochs\"]\n",
    "\n",
    "print(\"Loaded configuration:\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from dataset: (tensor([1, 3, 5, 6]), tensor([5, 2, 3]))\n"
     ]
    }
   ],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, num_samples=500, primes=[3, 5, 7, 11]):\n",
    "        self.primes = primes\n",
    "        self.P = 1\n",
    "        for p in primes:\n",
    "            self.P *= p\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            # Random integer N in [0, P)\n",
    "            N = torch.randint(0, self.P, (1,)).item()\n",
    "            # Input tokens: remainders for each prime (as integers)\n",
    "            input_tokens = [N % p for p in primes]\n",
    "            # Output tokens: digits of N (each as an integer)\n",
    "            output_tokens = [int(d) for d in str(N)]\n",
    "            self.samples.append((torch.tensor(input_tokens, dtype=torch.long),\n",
    "                                 torch.tensor(output_tokens, dtype=torch.long)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "# Quick test\n",
    "dataset = TranslationDataset(num_samples=10)\n",
    "print(\"Sample from dataset:\", dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)  # Create a (max_len, d_model) matrix.\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # Shape: (max_len, 1) with positions 0,1,2,...\n",
    "        # Compute a scaling factor for each even dimension.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # For even indices: use sine; for odd indices: use cosine.\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # Shape becomes (max_len, 1, d_model) for easy broadcasting.\n",
    "        self.register_buffer('pe', pe)  # Register as a buffer so itâ€™s part of the module but not a parameter.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_len, batch_size, d_model)\n",
    "        x = x + self.pe[:x.size(0)]  # Add positional encoding to each token embedding.\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original token matrix:\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "\n",
      "Positional encodings (first 4 positions):\n",
      "tensor([[[0., 1., 0., 1.]]])\n",
      "\n",
      "Token matrix after adding positional encoding:\n",
      "tensor([[[0., 1., 0., 1.],\n",
      "         [0., 1., 0., 1.],\n",
      "         [0., 1., 0., 1.],\n",
      "         [0., 1., 0., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# Test parameters\n",
    "d_model = 4      # Dimensionality of embeddings/positional encodings\n",
    "seq_len = 1      # Sequence length\n",
    "batch_size = 4   # Batch size\n",
    "\n",
    "# Create a dummy token matrix (for example, all zeros)\n",
    "dummy_tokens = torch.zeros(seq_len, batch_size, d_model)\n",
    "print(\"Original token matrix:\")\n",
    "print(dummy_tokens)\n",
    "\n",
    "# Instantiate PositionalEncoding with no dropout for clarity\n",
    "pos_enc = PositionalEncoding(d_model, dropout=0.0, max_len=10)\n",
    "\n",
    "print(\"\\nPositional encodings (first 4 positions):\")\n",
    "# The positional encoding matrix has shape (max_len, 1, d_model)\n",
    "# We'll print the first 4 positions, which correspond to our sequence length.\n",
    "print(pos_enc.pe[:seq_len])\n",
    "\n",
    "# Add positional encoding to the dummy tokens\n",
    "tokens_with_pe = pos_enc(dummy_tokens)\n",
    "print(\"\\nToken matrix after adding positional encoding:\")\n",
    "print(tokens_with_pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq Transformer model\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=64, nhead=4,\n",
    "                 num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=128, dropout=0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        # Embedding layers for source (moduli) and target (digits).\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        # Positional encodings for source and target.\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.pos_decoder = PositionalEncoding(d_model, dropout)\n",
    "        # Transformer module from PyTorch.\n",
    "        self.transformer = nn.Transformer(d_model, nhead,\n",
    "                                          num_encoder_layers, num_decoder_layers,\n",
    "                                          dim_feedforward, dropout)\n",
    "        # Final linear layer maps transformer output to target vocabulary logits.\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        # Expect src and tgt shapes: (batch_size, seq_len)\n",
    "        # Transpose to shape: (seq_len, batch_size) as required by the transformer.\n",
    "        src = src.transpose(0, 1)\n",
    "        tgt = tgt.transpose(0, 1)\n",
    "        # Obtain token embeddings and apply scaling.\n",
    "        src_emb = self.src_embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        # Add positional encodings.\n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "        tgt_emb = self.pos_decoder(tgt_emb)\n",
    "        # Forward pass through the transformer.\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        # Project transformer outputs to logits and transpose back to (batch_size, seq_len, vocab_size).\n",
    "        logits = self.fc_out(outs)\n",
    "        return logits.transpose(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "# For instance, suppose our source vocabulary (mod values) is 0..11 and target vocabulary (digits) is 0..9 plus a special token.\n",
    "src_vocab_size = 12   # e.g., mod values 0,1,...,11\n",
    "tgt_vocab_size = 11   # e.g., digits 0-9 plus a special token (like BOS)\n",
    "\n",
    "# Instantiate the model.\n",
    "model = Seq2SeqTransformer(src_vocab_size, tgt_vocab_size, d_model=64, nhead=4,\n",
    "                           num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=128, dropout=0.1)\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
